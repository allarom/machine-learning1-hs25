---
title: "Used Cars – Applied ML Project"
output: html_document
---

<!-- Load libraries -->
```{r setup, message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(here)
```

# About the Dataset (Summary)

The Used Car Price Prediction dataset contains 4,009 vehicle listings collected from the automotive marketplace cars.com.
Each row represents a unique car and includes nine key attributes relevant to pricing and vehicle characteristics.
Dataset is taken from Kaggle: https://www.kaggle.com/datasets/taeefnajib/used-car-price-prediction-dataset

The dataset provides information on:

**Brand and model** – manufacturer and specific vehicle model

**Model year ** – age of the car, influencing depreciation

**Mileage** – an indicator of usage and wear

**Fuel type** – e.g., gasoline, diesel, electric, hybrid

**Engine type** – performance and efficiency characteristics

**Transmission** – automatic or manual

**Exterior/interior colors** – aesthetic properties

**Accident history** – whether the car has previously been damaged

**Clean title** – legal/ownership status

**Price** – listed price of the vehicle

Overall, the dataset offers a structured overview of key features that influence used car valuation. It is well-suited for analytical tasks such as understanding pricing drivers, exploring consumer preferences, and building predictive models for vehicle prices.
# Raw data

We load the original CSV directly from the project data folder using `here()` so paths work regardless of the working directory.

```{r load-raw, message=FALSE}
raw_path <- here("data", "raw", "used_cars.csv")
cars_raw <- readr::read_csv(raw_path, show_col_types = FALSE)
```

Basic structure and summary statistics of the raw dataset:

```{r raw-summary}
glimpse(cars_raw)
```

# Exploratory Data Analysis

We base the EDA on the engineered dataset (`data/processed/used_cars_features.csv`) that keeps cleaned numeric fields and derived features like age, mileage in thousands, and accident flags.

```{r load-features, message=FALSE}
features_path <- here("data", "processed", "used_cars_features.csv")
cars <- readr::read_delim(features_path, delim = ";", show_col_types = FALSE)
```

## Key descriptive values

```{r descriptive-table, message=FALSE, echo=FALSE}
num_summary <- tibble::tibble(
  variable = c("price_dollar", "log_price", "age", "milage_k", "horsepower"),
  median   = c(median(cars$price_dollar), median(cars$log_price), median(cars$age), median(cars$milage_k), median(cars$horsepower)),
  mean     = c(mean(cars$price_dollar), mean(cars$log_price), mean(cars$age), mean(cars$milage_k), mean(cars$horsepower)),
  p25      = c(quantile(cars$price_dollar, 0.25), quantile(cars$log_price, 0.25), quantile(cars$age, 0.25), quantile(cars$milage_k, 0.25), quantile(cars$horsepower, 0.25)),
  p75      = c(quantile(cars$price_dollar, 0.75), quantile(cars$log_price, 0.75), quantile(cars$age, 0.75), quantile(cars$milage_k, 0.75), quantile(cars$horsepower, 0.75)),
  sd       = c(sd(cars$price_dollar), sd(cars$log_price), sd(cars$age), sd(cars$milage_k), sd(cars$horsepower)),
  min      = c(min(cars$price_dollar), min(cars$log_price), min(cars$age), min(cars$milage_k), min(cars$horsepower)),
  max      = c(max(cars$price_dollar), max(cars$log_price), max(cars$age), max(cars$milage_k), max(cars$horsepower))
)

knitr::kable(num_summary, digits = 2, caption = "Key numeric feature summaries")
```

```{r accident-summary, message=FALSE, echo=FALSE}
accident_tbl <- as.data.frame(table(cars$accident), stringsAsFactors = FALSE) |>
  dplyr::rename(accident = Var1, n = Freq) |>
  dplyr::mutate(share = n / sum(n))

knitr::kable(accident_tbl, digits = 2, caption = "Accident history distribution")
```

Median listing sits around \$28k, with the middle 50% between roughly \$15.5k and \$47k, while the maximum reaches \$650k—explaining the heavy right tail. Median age is 9 years (IQR: 6–14), typical mileage is about 63k miles (IQR: 30k–103k), and horsepower clusters around 310 HP (IQR: 248–400). About 28% of cars report an accident or damage, a meaningful factor for pricing.

## Price distribution (raw and log)

```{r price-raw, echo=FALSE, out.width="85%"}
knitr::include_graphics(here("report", "plots", "price_distribution_raw.png"))
```

Raw prices are extremely right-skewed, with most listings below \$80k but a long tail of luxury and exotic vehicles. Modeling on this scale would be dominated by a few high-price outliers.

```{r price-log, echo=FALSE, out.width="85%"}
knitr::include_graphics(here("report", "plots", "price_distribution.png"))
```

Log transformation produces a more bell-shaped distribution and stabilizes variance, making linear-style models and visual comparisons more reliable.

## Depreciation by age and fuel type

```{r age-price, echo=FALSE, out.width="85%"}
knitr::include_graphics(here("report", "plots", "age_vs_price.png"))
```

Prices decline with age across fuels. Electric listings start high but show the sharpest early drop; diesel holds comparatively high prices across ages (though the diesel sample is small), and gasoline sits lower overall.

## Price spread across top brands

```{r brand-price, echo=FALSE, out.width="85%"}
knitr::include_graphics(here("report", "plots", "brand_price_boxplots.png"))
```

Among the 12 most common brands, Porsche leads on median price, followed by Land Rover and Mercedes-Benz; Volume brands (Toyota, Nissan, Jeep) cluster lower with tighter spreads, while some (Chevrolet, Ford) span broader lineups.

## Mileage impact by transmission

```{r milage-price, echo=FALSE, out.width="85%"}
knitr::include_graphics(here("report", "plots", "milage_vs_price.png"))
```

Higher mileage correlates with lower prices. We use a loess smoother (not a straight trendline) and cap the x-axis at 250k miles to reduce the influence of extreme outliers; automatics show a steady decline, and the smaller manual subset is noisier but similar in direction.

## Horsepower premium

```{r hp-price, echo=FALSE, out.width="85%"}
knitr::include_graphics(here("report", "plots", "horsepower_vs_price.png"))
```

Price rises with horsepower, especially from ~250 HP upward; we cap horsepower at 700 to avoid a handful of ultra-high-HP outliers from distorting the loess smoother, so the trend reflects the bulk of the market rather than extreme sports models.

## Accident history effect

```{r accident-price, echo=FALSE, out.width="70%"}
knitr::include_graphics(here("report", "plots", "accident_vs_price.png"))
```

Cars with reported accidents trade at a clear discount relative to clean histories, even after log-scaling prices, confirming accident history as an important predictor.

## SVM model results

We fit radial-kernel SVM regressors on `log_price` using both `e1071::svm` and `kernlab` (via `caret::train`). Both models use the same 80/20 train-test split; hyperparameters are tuned by cross-validation and evaluated on the hold-out test set below.

```{r svm-metrics, message=FALSE}
svm_metrics <- readr::read_csv(
  here("report", "models", "svm", "svm_log_price_metrics.csv"),
  show_col_types = FALSE
)

svm_best <- readr::read_lines(here("report", "models", "svm", "svm_best_model.txt"))[1]

svm_metrics_wide <- svm_metrics |>
  tidyr::pivot_wider(names_from = .metric, values_from = .estimate)

knitr::kable(svm_metrics_wide, digits = 3, caption = "Test metrics for SVM variants (target: log_price)")
```

The `e1071` radial SVM is best by RMSE (~0.290) and R² (~0.868), outperforming the kernlab variant on this split. SVMs do not yield straightforward coefficient interpretations; they learn support vectors and decision functions in a transformed feature space. To understand feature effects you would rely on downstream tools (e.g., partial dependence or SHAP), but within this report we focus on comparative error metrics and note that the tuned radial kernel captures non-linear relationships beyond the linear/log models. 

Cross-validation setup: both SVMs were tuned with 3-fold cross validation on the training set (same 80/20 split for both). `e1071::tune()` searched a compact grid of `cost` and `gamma`; `caret::train(method = "svmRadial")` searched a grid of `C` and `sigma`. Final metrics shown above are from the untouched test set, so cross validation was only for hyperparameter selection.

`r svm_best`

```{r svm-pred-vs-true, fig.width=11, fig.height=5.5, message=FALSE, echo=FALSE}
pred_e1071 <- readr::read_delim(
  here("report", "models", "svm", "svm_log_price_e1071_predictions.csv"),
  delim = ";",
  show_col_types = FALSE
) |>
  dplyr::mutate(model = "e1071_radial")

pred_kern <- readr::read_delim(
  here("report", "models", "svm", "svm_log_price_kernlab_predictions.csv"),
  delim = ";",
  show_col_types = FALSE
) |>
  dplyr::mutate(model = "kernlab_radial")

plot_svm <- function(df, title) {
  df <- df |>
    dplyr::mutate(resid = pred_log_price - log_price)

  ggplot2::ggplot(df, ggplot2::aes(x = log_price, y = pred_log_price, color = resid)) +
    ggplot2::geom_point(alpha = 0.7, size = 1.4) +
    ggplot2::geom_smooth(method = "loess", se = FALSE, color = "black", linewidth = 0.8) +
    ggplot2::geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray40") +
    ggplot2::coord_equal() +
    ggplot2::scale_color_gradient2(
      low = "#B91C1C", mid = "#CBD5E1", high = "#15803D",
      midpoint = 0,
      name = "Residual price (predicted  - true)\nred = under-prediction, grey = correct, green = over-prediction"
    ) +
    ggplot2::labs(
      title = title,
      x = "True log(price)",
      y = "Predicted log(price)"
    ) +
    ggplot2::theme_minimal(base_size = 12) +
    ggplot2::theme(
      legend.position = "none",
      plot.title.position = "plot",
      plot.title = ggplot2::element_text(hjust = 0, vjust= -20)
    )
}

p1 <- plot_svm(pred_e1071, "e1071 radial SVM:\n predicted vs true")
p2 <- plot_svm(pred_kern, "kernlab radial SVM:\n predicted vs true")

# Standalone legend using the same residual scale
legend_plot <- ggplot2::ggplot(
  data.frame(resid = c(-0.5, 0, 0.5), x = 1, y = 1),
  ggplot2::aes(x = x, y = y, color = resid)
) +
  ggplot2::geom_point(alpha = 0) + # invisible points; legend only
  ggplot2::scale_color_gradient2(
    low = "#B91C1C", mid = "#CBD5E1", high = "#15803D",
    midpoint = 0,
    name = "Residual price (predicted  - true)\nred = under-prediction, grey = correct, green = over-prediction"
  ) +
  ggplot2::guides(color = ggplot2::guide_colorbar(barheight = unit(60, "pt"))) +
  ggplot2::theme_void() +
  ggplot2::theme(legend.position = "right")

patchwork::wrap_plots(
  p1,
  p2,
  legend_plot,
  ncol = 3,
  widths = c(1, 1, 0.22)
)
```
